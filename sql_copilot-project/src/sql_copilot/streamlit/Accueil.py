# Accueil.py
from __init__ import *
import torch
import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import os

base_path = r"C:\Users\ksimotabudjifupa\Documents\Personnel\Data_science_projects\sql_copilot"

st.title("***Text To SQL ü§ñ***")

if "model_name" not in st.session_state:
    st.session_state.model_name = None
if "tokenizer" not in st.session_state:
    st.session_state.tokenizer = None
if "trainable_model" not in st.session_state:
    st.session_state.trainable_model = None

@st.cache_resource(show_spinner=True)
def llm():
    # Paths to model directories
    base_model_path = os.path.join(base_path, "models", "base_Phi2_model")
    peft_model_path = os.path.join(base_path, "models", "Text2SQL_Phi2_model")

    # Make sure paths exist
    if not os.path.isdir(base_model_path):
        raise FileNotFoundError(f"Base model path does not exist: {base_model_path}")
    if not os.path.isdir(peft_model_path):
        raise FileNotFoundError(f"PEFT model path does not exist: {peft_model_path}")

    # Quantization config
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    # Load base model
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        quantization_config=bnb_config,
        trust_remote_code=False,
        device_map="auto"
    )

    # Load PEFT fine-tuned model
    trainable_model = PeftModel.from_pretrained(
        base_model,
        peft_model_path,
        is_trainable=False  # Set to False unless you are training
    )

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        peft_model_path,
        trust_remote_code=True,
        use_fast=False
    )

    model_name = "Phi2 Text To SQL"
    return model_name, tokenizer, trainable_model

# UI Components
st.subheader(":blue[***Description du projet***]", divider="blue")

# Cache model loading in session state
st.session_state.model_name, st.session_state.tokenizer, st.session_state.trainable_model = llm()

# Project Description
with st.container():

    st.markdown(
        "ü§ñ **Text to SQL** est un projet d‚Äôintelligence artificielle (IA) permettant de g√©n√©rer et d‚Äôex√©cuter des requ√™tes SQL √† partir d‚Äôinstructions en langage naturel."
    )

    st.markdown("### üîß √âtapes cl√©s du d√©veloppement :")

    st.markdown("1Ô∏è‚É£ **Constitution de la base d'entra√Ænement** : utilisation des jeux de donn√©es *Spider* et *WikiSQL* ;")
    st.markdown("2Ô∏è‚É£ **Choix du mod√®le LLM** : s√©lection de *Phi-2* de Microsoft pour sa l√©g√®ret√© et ses performances ;")
    st.markdown("3Ô∏è‚É£ **M√©thode de fine-tuning** : utilisation de *LoRA* (*Low-Rank Adaptation*) pour un affinement efficace ;")
    st.markdown("4Ô∏è‚É£ **Base de donn√©es fictive** : cr√©ation de 6 tables pour l'interaction avec le mod√®le personnalis√©.")



st.info(f"Le mod√®le de Text to SQL est : **{st.session_state.model_name}**")

db_path = os.path.join(base_path, "database", "crm.db")
db_connection_status = check_database_connection(db_file=db_path)
if db_connection_status == "non connect√©e":
    st.error("***Impossible de se connecter √† la base de donn√©es***")
    st.toast("***Connexion base de donn√©es impossible***", icon="‚õî")
else:
    st.toast("***Connexion base de donn√©es effectu√©e***", icon="üì∂")

banner_bottom()
